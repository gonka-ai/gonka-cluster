---
# HuggingFace Model Download Role
# Download all required models to shared cache

- name: Download required models
  shell: |
    . /opt/gonka-venv/bin/activate
    export HF_HOME={{ hf_home }}
    attempts={{ hf_download_retries | default(3) }}
    i=1
    while [ $i -le $attempts ]; do
      echo "[HF] Download {{ item.name }} (attempt $i/$attempts)"
      if huggingface-cli download {{ item.name }} --resume; then
        exit 0
      fi
      sleep $(( 30 * i ))
      i=$(( i + 1 ))
    done
    echo "[HF] Failed to download {{ item.name }} after $attempts attempts"
    exit 1
  args:
    executable: /bin/bash
  environment:
    HF_TOKEN: "{{ lookup('env', 'HF_TOKEN') | default(omit) }}"
    HF_HUB_ENABLE_HF_TRANSFER: "{{ hf_hub_enable_hf_transfer | default(omit) }}"
  loop: "{{ model_list.small + model_list.medium + model_list.large }}"
  async: "{{ model_download_timeout }}"
  poll: 20
  register: download_result

- name: Validate HF cache presence for each model
  shell: |
    base="{{ hf_home }}/hub/models--{{ item.name | replace('/', '--') }}"
    if [ ! -d "$base" ]; then exit 2; fi
    # Prefer config.json in snapshots; otherwise accept any blob presence
    if find "$base/snapshots" -type f -name config.json 2>/dev/null | head -n1 | grep -q .; then exit 0; fi
    if find "$base/blobs" -type f 2>/dev/null | head -n1 | grep -q .; then exit 0; fi
    exit 1
  args:
    executable: /bin/bash
  loop: "{{ model_list.small + model_list.medium + model_list.large }}"
  register: hf_cache_check
  changed_when: false
  failed_when: false

- name: Assert models present in HF cache
  assert:
    that: item.rc == 0
    fail_msg: "Model {{ item.item.name }} not found in HF cache under {{ hf_home }}/hub/models--ORG--REPO/{snapshots|blobs}"
  loop: "{{ hf_cache_check.results | default([]) }}"

- name: Calculate total downloaded size
  shell: >
    . /opt/gonka-venv/bin/activate &&
    du -sh {{ hf_home }}
  args:
    executable: /bin/bash
  register: total_size
  changed_when: false

- name: Display download completion
  debug:
    msg: |
      ✅ Model downloads completed on {{ inventory_hostname }}
      📊 Total cache size: {{ total_size.stdout }}
      📁 Cache location: {{ hf_home }}
      🧠 Models ready for inference
